# 私有化 Dify + 大模型部署与调优指南

本文档详细说明如何构建一套完全私有化、高准确率、低幻觉的企业级知识库问答系统。

## 1. 整体架构设计

为保证数据完全不出内网，我们需要构建以下三层架构：

```mermaid
graph TD
    User[前端用户 (React App)] -->|API 调用| Dify[中间件: Dify 私有化平台]
    Dify -->|文档存储| Storage[存储: MinIO / 本地磁盘]
    Dify -->|向量检索| VectorDB[向量库: Weaviate / Milvus]
    Dify -->|模型调用| Inference[推理引擎: Xinference / Ollama]
    Inference -->|显卡加速| GPU[硬件: NVIDIA GPUs]
```

## 2. 私有化部署核心组件

### 2.1 Dify (大脑指挥官)
*   **部署方式**: 推荐使用官方提供的 Docker Compose 一键部署。
*   **数据存储**: 默认使用 Postgres (业务数据) + Redis (缓存) + 本地文件系统 (上传的文件)。

### 2.2 推理引擎 (模型驱动)
*   **推荐工具**: **Xinference** (推荐) 或 Ollama。
*   **理由**: Xinference 对 Dify 支持最好，且原生支持 Rerank（重排）模型，这对减少幻觉至关重要。

## 3. 模型选择：如何确保无幻觉？

要消除“胡说八道”的幻觉，关键不在于模型有多大，而在于**“检索增强 (RAG)”**的质量。你需要部署两个模型：

### A. 思考模型 (LLM) - 负责组织语言
*   **首选**: **Qwen 2.5-72B-Instruct** (通义千问 2.5)
    *   *特点*: 中文理解能力目前开源界最强，逻辑严密，听话（指令遵循能力强）。
    *   *硬件需求*: 至少 2 张 A100/A800 (80G) 或 4 张 3090/4090 (量化版)。
*   **高性价比备选**: **Qwen 2.5-14B-Instruct**
    *   *特点*: 能够在单张 4090 (24G) 显卡上流畅运行，效果在大多数垂直领域足够好。

### B. 排序模型 (Rerank Model) - 负责精准检索 (关键!)
这是私有化部署中**最容易被忽视**但**最重要**的一环。
*   **推荐**: **bge-reranker-v2-m3** 或 **bge-reranker-large**。
*   **作用**: Dify 初步检索出 20 段相关文字后，Rerank 模型会像老师改卷子一样，给这 20 段文字打分，把真正相关的 3-5 段挑出来喂给大模型。
*   **效果**: 加了 Rerank，检索准确率通常能提升 30% 以上，直接扼杀幻觉根源。

## 4. 文件上传系统方案

私有化部署后，文件存储有两种主要方案：

### 方案 A：默认本地存储 (中小规模推荐)
*   **原理**: Dify 的 Docker 容器会挂载宿主机的一个目录 (如 `/app/api/storage`)。
*   **优点**: 零配置，部署即用。
*   **缺点**: 只能单机部署，无法多台服务器共享。
*   **适用**: 单机私有化部署。

### 方案 B：MinIO 对象存储 (企业级推荐)
*   **原理**: 部署一个 MinIO 服务（兼容 AWS S3 协议的私有化对象存储）。
*   **配置**: 在 Dify 的 `docker-compose.yaml` 或环境变量中配置 `STORAGE_TYPE=s3` 及 MinIO 的连接信息。
*   **优点**:
    *   **无限扩容**: 硬盘满了加硬盘即可。
    *   **高性能**: 支持大文件断点续传。
    *   **统一管理**: 所有上传的 PDF、图片都在 MinIO 里，方便备份和迁移。

## 5. 实施路线图

1.  **硬件准备**: 准备一台 Linux 服务器，安装 Docker 和 NVIDIA Driver。
2.  **部署推理服务**:
    ```bash
    # 示例：使用 Xinference 启动 Qwen2.5 和 Rerank 模型
    xinference-local --host 0.0.0.0
    # 在 WebUI 中启动 qwen2.5-instruct 和 bge-reranker
    ```
3.  **部署 Dify**:
    *   下载 Dify 源码。
    *   修改 `.env` 文件，配置存储方式 (Local/MinIO)。
    *   `docker compose up -d`
4.  **配置连接**:
    *   在 Dify 后台 -> 设置 -> 模型供应商 -> Xinference。
    *   填入地址，添加 LLM 和 Text Embedding / Rerank 模型。
5.  **构建知识库**:
    *   上传文档 -> 选择 "混合检索" (Hybrid Search) -> 开启 "N选1召回" (Rerank)。
    *   设置 Rerank 分数阈值为 0.6 (低于此分数的参考资料不给大模型看，防止干扰)。

通过这套组合拳 (Qwen 2.5 + BGE Rerank + 混合检索)，可以最大限度地确保回答基于事实，杜绝幻觉。
